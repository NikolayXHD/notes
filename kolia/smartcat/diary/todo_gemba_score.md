# Сохранение оценки качества перевода ревизий сегментов GEMBA score

## Область применения

- В рамках данной задачи сохраняем GEMBA score прямо в ревизию сегмента
- Нигде не используем
- В будущих задачах GEMBA score будет использован
  - в продукте smartcat, будем показывать качество перевода в UI проекта / документа,
    агрегируя данные по индивидуальным сегментам в рамках направления перевода
  - в продуктовых исследованиях, будем делать выгрузку в smartcat_stats
    для бизнес-аналитики

## Сохраняемые признаки

```
AccountId
ProjectId
DocumentId
SegmentId
TargetLanguageId
RevisionId
WorkflowStageNumber

UserId
TmQualityPercentage
MtKey
MtPresetId

Score
```

## Выбор ревизий для оценивания

Задача на оценку качества перевода всегда оценивает
- последнюю подтверждённую ревизию сегмента
- последнюю среди TM ревизий, следующих за последней подтверждённой ревизией
- последнюю среди MT ревизий, следующих за последней подтверждённой ревизией
- если подтверждённых ревизий нет, смотрим на последнюю TM ревизию и последнюю MT ревизию

## Управляемость сбора статистики

Строим процесс расчёта статистики с учётом требований к его управляемости.

## Прототип, 1й этап

Для экспериментирования, нам необходима дополнительная управляемость, 
возможность инициировать перерасчёт в ограниченном scope:

1. Для отдельной организации (включает >= 1 корп аккаунта)
2. Для отдельного корп. аккаунта (workspace)
3. Для отдельного для отдельного ProjectId (+ TargetLanguageId, optional)
4. Для отдельного DocumentId (+ TargetLanguageId, optional)

## Прототип, 2й этап.

Дополнительные признаки заадчи на расчёт статистики.

### `sampling%`

Ставя задачу на расчёт статистики в некотором scope, напр.
`ProjectId + TargetLanguageId`, мы захотим проверять не все, а определённую долю
сегментов

### `score_old_revisions`

По дефолту мы оцениваем, грубо говоря последнюю ревизию, см. точнее 
`Выбор ревизий для оценивания`.

При наличии в задаче признака `score_old_revisions`, мы должны оценить
все TM / MT ревизии, а также все подтверждённые ревизии, а не только
последние.

## Боевой вариант

В конечном итоге, мы должны придти к полностью автоматическому механизму, когда
мы рассчитываем score для всех подтверждённых ревизий, а также всех TM и MT
ревизий, независимо от признака подтверждения.

## Механизм управления

С учётом требований у управляемости на этапе прототипа, представляется разумным
построить единый целостный механизм постановки задач, поддерживающий все scope
расчёта TQS.

По мере приближения к боевому варианту, ненужные механизмы будем отбрасывать.

1. Делаем сложный TaskManager, `TranslationQualityScoreTaskManager` позволяющий
   ставить задачи на перерасчёт с разным scope.
2. Автоматические триггеры расчёта статистики приводят к постановке новых задач
  через `TranslationQualityScoreTaskManager`. На 1м этапе прототипа не делаем.
  2.1. Подтверждение сегмента
  2.2. создание MT / TM ревизии, независимо от признака подтверждения. Потребуется дополнительное событие Kafka

## UI механизма управления

Каким образом должны появляться задачи для `TranslationQualityScoreTaskManager`?

Подход 1. Создавать руками в prod базе

- опасно, кто-то будет часто держать открытое подключение с write permission
- неудобно, придётся постоянно дёргать разработчика

Подход 2. Internal API, swagger
Выглядит разумным компромиссом.
- Не нужно костылять одноразовый UI
- Есть какой-никакой UI с документацией, что делает метод и на что влияют параметры
- Есть валидация запроса на стороне C# бэкенда, swagger покажет подробности BadRequest
- Нельзя по ошибке записать в prod базу невалидный объект задачи
- Нельзя нанести другой случайный урон, по через открытое подключения на запись к базе.
Обсудил. Не подходит, т.к. swagger на prod есть только во внешнем интеграционном API.
Не стоит выставлять нашим корп пользователям внутренний экспериментальный функционал.


Подход 3. UI в админке
- Долго разрабатывать

В качестве компромисса, можно сделать API админки без UI
Запросы шлём вручную через Postman или Curl, скопировав из браузера куку
`"session"` после авторизации в админке


https://smartcat1.slack.com/archives/GC4J30KV4/p1689621004210099

## Механизм расчёта

1. Мы оцениваем каждую ревизию таргета по отдельности
2. Чтобы снизить накладные расходы на prompt ChatGPT мы отправляем запрос на оценивание
   нескольких сегментов сразу.
3. Мы ожидаем, что в случае, когда сегменты в пачке, которую мы помещаем в 1
   шаблон prompt ChatGPT, идут подряд, качество оценки выше. Поэтому:
  3.1. рассчитывая score для крупных единиц (Project, Document) мы складываем
       ревизии по порядку следования сегментов в пределах `DocumentId + TargetLanguageId`
  3.2. При необходимости сосчитать score 1 индивидуального сегмента, мы дополняем prompt
       соседними сегментами слева и справа, при условии что у них уже есть перевод.

Разработка
----------

Оцениваем индивидуальные ревизии сегмента без контекста.
- Оценивать будем сегменты независимо от длины, но это может измениться
  после исследования Оценка качества.
  - Возможный вариант, оценивать sampling % самых длинных сегментов по source тексту

В задаче Оценка качества будет выбраны развилки
- Вилка 1
  - [v] ChatGPT
    - Из за сжатых сроков примем что используется ChatGPT

    - Используем существующий конфиг OpenAI с шаблонами промтов и Token
      дорабатываем структуру данных конфига по необходимости

    - Применение лимита запросов и токенов в мин на стороне smartcat
      - токены нельзя посчитать из c#, их нет в API GPT
      - Можно грубо оценивать 4 буквы = 1 токен
        # https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them
        # 1 token ~= 4 chars in English

    - Расход лимита OpenAI, запрос на повышение лимита
      Сколько точно понадобится будет зависить от выбранного механизма оценивания, см
      ниже Оценка качества
      - 1_500 / 3_500 запросов в мин, если 1 запрос на ревизию
      - 50_000 / 90_000 токенов в мин, не считая prompt template

  - BLURT

- Вилка 2
  - Запрос батчем, несколько ревизий
    - [v] Из за сжатых сроков примем, что отправляем запрос батчем.
  - 1 запрос на 1 ревизию

- Фичафлаг аккаунта
- Таскпроцессинг по аналогии с EditStatisticsProcessor
  - перебирает раз в день, смотрит на дату подтверждения сегмента
  - смотрит и сохраняет качество
    - только для подтверждённых человеческих авторов
    - MT / TM, игнорируя признак подтверждённости
  - работает только для корп аккаунтов
    - учитывает фичафлаг корп аккаунта
  - кастомное поле project_ids в задаче
    захватив задачу с непустыми project_ids, процессинг пересчитает стату только для этих проектов,
    игнорируя время подтверждения ревизии и аккаунт
    - кастомный параметр % семплинга для проекта
  - захардкоженный дефолтный % семплинга в обработчике задачи


- [v] Посмотреть как реализовано прикапывание QA оценок
  позволит ответить на вопрос, как мы прикапываем в ревизии сегмента результаты
  тяжёлых расчётов
  - QA оценки прикапываются на уровне Segment.Target, а не ревизии

Оценка качества
---------------

Текущая исходная посылка, что мы стремимся оценивать качество на уровне отдельных сегментов.
Но сегменты зачастую короткие, несколько слов.
Нужно проверить страдает ли качество GEMBA score для мелких текстов.

Какие измерения я бы хотел получить от исследования промтов.

1. Что будет, если мы просим GPT оценить качество текста длиной ~300 токенов, но просим
   выдать отдельную оценку для каждого сегмента (предложения).
   1.1. Как варьируется качество GEMBA score в зависимости от длины предложения
        Должны быть примеры с очень маленькими предложениями, <= 5 слов
   1.2. от позиции в списке предложений целого текста 1-е предложение, 2-е и т.д.

2. Насколько страдает качество GEMBA score, если сегменты, которые нужно оценить,
   не следуют подряд в исходном тексте. Например, Есть исходный текст,
   поделённый на сегменты, и мы оцениваем каждый 4-й сегмент.

От ответов на эти вопросы будет зависить дизайн sampling-а текстов для проверки и сохранения оценки.
Если например мы не можем получить приемлемое качество оценки отдельных сегментов, то придётся хранить
оценку для документа целиком.

