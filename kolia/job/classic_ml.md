Обзор задач ML
"/run/media/kolia/ssd/book/ml/I. Goodfellow, Y. Bengio, A. Courville - Deep Learning - 2016.pdf"
глава Machine Learning Basics

mutual information
------------------
- оценивает связь между ровно 1 независимой переменной и целевой переменной
- в отличие от корреляции, применим к любой функциональной зависимости,
  линейность не требуется
- нужно понимать, что низкое значение MI не гарантирует, что переменная
  плохая, т.к. переменная может описывать взаимодействие. 
  напр параболоид `y = x1 ** 2 + x2 ** 2`
- формула для дискретного случая
  ```
  sum(
    p(x,y)
    * log(
      p(x, y)
      / ( p(x) * p(y) )
    )
  )
  ```
- 0 тогда и т.т.к. переменные независимы

pairwise distance correlation
-----------------------------
- в отличие от корреляции, применим к любой функциональной зависимости,
  линейность не требуется
- 0 т и т.т.к. переменные независимы
- Может оценивать пару и большее количество независимых переменных

tSNE
----
t-distributed Stochastic Neigbour Embedding
- Снижение размерности без предположения линейности
- Стремится сохранить расстояния

principal component analisys
----------------------------
https://www.sartorius.com/en/knowledge/science-snippets/what-is-principal-component-analysis-pca-and-how-it-is-used-507186#:~:text=Principal%20component%20analysis%2C%20or%20PCA,more%20easily%20visualized%20and%20analyzed
https://mlcourse.ai/book/topic07/topic7_pca_clustering.html#principal-component-analysis-pca
Оси
  - каждая ось - линейная комбинация исходных независимых переменных
  - являются собственными векторами матрицы корреляции в порядке убывания
    собственного значения
  - переменные, соотвествующие осям ортогональны (не коррелируют)
  - будет выдано столько же осей, сколько независимых переменных
  - оси идут в порядке убывания разброса целевой переменной
PCA score plot
  - под словом score подразумеваются просто оси PCA
    score1 - координата наблюдения вдоль первой оси PCA
  - можно отобразить наблюдения на на 2D, где x, y, первые 2 оси PCA
  - рядом окажутся наблюдения с похожими значениями 2х главных осей
PCA loading plot
  - нарисуем 2D scatter, где каждая точка это фича, а её координаты -- веса
    в двух главных осях PCA.
  - Фичи близко к началу координат мало влияют на главные оси
  - Коррелирующие фичи имеют похожее направление, и наоборот

Визуальный анализ данных
------------------------
https://mlcourse.ai/book/topic02/topic02_visual_data_analysis.html

- Матрица корреляции
  ```
  df[feature_list].corr()
  ```
  можно отбросить де-факто дублирующие переменные, где корреляция с другой переменной
  близка к 1
- sns.jointplot
  2D scatter + гистограммы на осях X, Y
- sns.pairplotЯ отвезу, но мне нужно оставить хотя бы
  матрица 2D графиков, пары фич
  логично использовать с hue=y

- sns.catplot
  матрица графиков
  col=переменная_варьирующаяся_вдоль_столбцов_матрицы
  row=переменная_варьирующаяся_вдоль_строк_матрицы

- sns.countplot
  - построит гистограмму для дискретной фичи
  - удобно в например сочетании с hue=y, будет несколько гистограмм
    для разных значений y

Дерево решений
--------------

Рекурсивно разбивает наблюдения на 2 группы, по граничному значению одной из
фич, на каждом разбиении жадно оптимизируя некоторую метрику.

Критерии разбиения
- Information Gain. Как меняется до и после разбиения
  средняя энтропия по листьям, вес = доля наблюдений в листе
  ```
  Sum(Ni / N * Si)
  ```
  i перебирает листья
  ```
  Si = - Sum(Pk * log(Pk))
  ```
  k перебирает наблюдения в 1 листе
  Pk вероятность индивидуального наблюдения

- Критерий Gini
  Заменяем энтропию `Si` на Gini impurity (aka Gini uncertainty) `G = Sum(Pk ** 2)`

В варианте регрессии, предсказываем константу в пределах листа,
минимизируем среднеквадратичное отклонение.

Плюсы
- Интерпретабельный
- Быстрое обучение и инференс
- Терпимо к дубликатам фич / сильно зависимым переменным
- Не требует масштабирования
- Толерантность к большому количеству фич
- Всеядность относительно непрерывных / дискретных фич

Минусы
- Неустойчивость к шумам и малым изменениям обучающей выборки
- Прямоугольная граница.
  - Где могло хватить простой линейной регрессии будет много-много уголков
  - Экстраполяция даст прямогуольные границы вне тренировочных данных, почти
    всегда это не ок


Knn
---
К ближайших соседей. Для больших данных потребуется векторная база.
Т.к. расстояние зависит от масштаба, фичи нужно предварительно стандартизовать.

Хороший и относительно быстрый baseline
Можно сочетать с кастомной метрикой расстояния или кастомным весом в зависимости
от расстояния

Плюсы
- Концептуально простой, интерпретабельный
- Достаточно гибкий с учётом кастомизации расстояния и веса
Минусы
- Медленнее Descision Tree, на этапе inference нужно искать соседей
- Не способно само отбросить дубликаты / сильно коррелирующие фичи

OLS Ordinary Least Squares
==========================

Минимизация среднеквадратичного отклонения
------------------------------------------

Рассматриваем ситуацию, когда зависимая переменная линейно связана с
независимыми, + некореллированный шум с 0 средним и одинаковой дисперсией

- Минизизация среднеквадратичного отклонения, даёт estimator с наименьшим variance
  среди всех линейных estimator с нулевым отклонением

Максимизация правдоподобия
--------------------------
- При соблюдении предположений выше, минимизация среднеквадратичного отклонения
  даёт тот же estimator, что максимизация правдоподобия (likelihood)

Разложение ошибки на Bias, Variance и неустранимую ошибку
---------------------------------------------------------

Размен повышения bias на снижение Variance
------------------------------------------
Сильно коррелирующие фичи приводят к близким к 0 собственным значениям,
XtX, считаем обратную матрицу, получаем очень большие числа

Другими словами, решение неустойчиво

Регуляризация
y = X * w
Предсказываем w
Loss = 1 / 2n Norm(y - Xw) ** 2 + Norm(w) ** 2


Регуляризация добавляет диагональную матрицу к почти вырожденной XtX, что уменьшает
её вырожденность

Как результат, получаем более устойчивое решение. Но т.к. мы штрафуем высокие w,
наше решение имеет Bias в сторону 0

Bagging
=======

Идея ансамблей
--------------
Классифицируем на основе majority vote
Регрессия на основе среднего предсказания
Работает в предположении некореллированности ошибок

Bagging
-------

- Bootstrapping. Набрать случайные выборки обучающего множества
  Каждый элемент выборки отбирается независимо, т.е. можно выбрать тот же
  элемент дважды.
- Обучить новую модель на каждой выборке
- Объединить предсказания

Out-of-bag error
----------------
В bagging происходит аналогичная кросс-валидации процедура, когда мы измеряем
ошибку классификатора на множестве наблюдений, которые он не видел, т.к. не 
попали в его bootstap выборку

Random Forest
-------------
- Набрать случайные выборки
- Для каждой выборки сделать случайную выборку фич
- Для каждой выборки обучить дерево до конца
  - Вариант Extreme Forest: выбирать точку разделения случайно.
    Делаем несколько случайных разбиений, выбираем лучшее.
- Объединить

Плюсы
-----
- Хорошо работает из коробки,
- Параллелизм
- Устойчивее к выборосам
- Не требует масштабирования фич
- Толерантность к большому количеству фич
- Всеядность относительно непрерывных / дискретных фич

Минусы
- Всё так же печально с экстраполяцией, всё такие же прямоугольные границы out-of-domain
- Предпочитает деления по категориям с высокой ординальностью
- Предпочитает фичи из более мелких взаимно-кореллированных групп

